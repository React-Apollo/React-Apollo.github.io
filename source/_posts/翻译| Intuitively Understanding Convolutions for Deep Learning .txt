---
title: 翻译|卷积网络的直觉理解
date: 2018-09-14 9:40:07
categories: Medium
tags: [ANN,Deep Learning]
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

_探索强视觉层级


近年来出现的强大,用途广泛的学习框架使得在深度学习模型中实现卷积层成为一件非常简单的任务了,经常通过一行代码就可以完成任务.

然而理解卷积,尤其是首次接触时,一些概念例如kernels,filter,channels 等等让人怀疑自己的智力水平了.但是,卷积作为一个概念,有着非常迷人的能力,还有高度扩展性.所以在本文中,我们将会详细的一步一步了解卷积操作的机制,把它关联到标准的全连接网络中,探讨一下,如何构建起一个强有力的视觉层级结构,使其成为图片特征的强有力提取工具.

## 2D 卷积: 操作

2D 卷积本质上是非常简单的操作:从一个kernel 开始,其中包括一个非常小的权重矩阵.这个 kernel 在2D 输入数据(平面图片)划过,和当前划过的部分执行点积(dot product),然后相加,输出单个像素.

[标准卷积图,文章中 2图]()

kernel 对于划过的每一个位置执行同样的操纵,把输入的2D特征矩阵转换为另一个2D 特征矩阵.输出矩阵是精简过的,输入特征的权重和(和 Kernel 的权重一起)对应着输出像素的同一位置. 

不管输入特征是否落在"大体统一位置".  (这里一句,先不翻译). 这意味着,kernel 的尺寸直接决定了多(少)输入特征联合产出一个新的输出特征.

这一点和全连接层形成鲜明的对比.在上面例子中,我们有 $5X5=25$ 个输入特征, $3X3=9$ 放入输出特征.如果这是一个全连接层,会有一个权重矩阵 $25X9=225$ 个参数, 每个输出特征对应着单个的输出特征.卷积可以做变换,仅仅只有9个参数.对于单个输出特征值,不是去"查看"每一个输入特征,仅仅来自于同一位置的输入特征.记住这一点,这是后续讨论的关键.

## 一些常见的使用技术


